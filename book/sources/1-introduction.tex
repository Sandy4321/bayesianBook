% !TEX root = ../script.tex
\section{Предисловие}

На русском языке существует несколько книг, 
которые затрагивают использование Байесовских методов машинному обучении.
В частности, можно порекомендовать учебное пособие Дмитрия Ветрова~\cite{vetrov2007bayesian} на русском языке.
Однако, до сих пор должного внимания не было уделено обоснованию Байесовского подхода --- в частности, 
с точки зрения классической математической статистики.

Данное учебное пособие призвано заполнить этот пробел в 
литературе, доступной на русском языке.
В первую очередь автор ориентировался на лекции Майкла Джордана, 
которые были прочитаны в Беркли в $2010$ году~\cite{jordan10notes}.
Получившийся результат во многом посвящен основному вопросу Байесовской статистики --- выбору априорного распределения,
хотя уделяет внимание и другим важным вопросам.

Оказывается, что Байесовская статистика использует 
многие фундаментальные понятия классической математической статистики и теории информации. 
Например, энтропию или теорию принятия решений.
Понимание принципов Байесовской статистики, таким образом, может оказаться полезным и в Байесовском машинном обучении, и для более глубокого понимания
современной математической статистики.

Авторы выражают благодарность Е.В.Бурнаеву за возможность чтения курса по Байесовским методам в ИППИ РАН, ВШЭ и Сколтехе.
Именно результатом опыта чтения курса и стало это пособие.

\section{Байесовский подход}

\subsection{Основные понятия}
Байесовский подход предлагает более гибкую трактовку традиционного вероятностного подхода.
Введем понятия, которые используются в Байесовской математической статистике и Байесовском машинном обучении.
Ограничимся параметрическими моделями в конечномерных пространствах.

Обычно нас интересует значение параметра или вектора параметров
$\vecT \in \Theta \subseteq \bbR^\pD$.
Мы оцениваем значение параметра по данным $X \in \mathcal{X} \subseteq \bbR^\iD$.
Для заданной вероятностной модели можно записать правдоподобие 
$p(X | \vecT)$.
Также задано априорное распределение $\pi(\vecT)$ для вектора параметров модели.

Помимо априорного распределения и правдоподобия определено
маргинальное распределение 
\[
p(X) = \int_{\Theta} p(X, \vecT) d\vecT = \int_{\Theta} p(X| \vecT) \pi(\vecT) d\vecT.
\]
Зная правдоподобие, апостериорное распределение и маргинальное распределение, мы можем записать
апостериорную плотность распределения $\vecT$:
\[
p(\vecT | X) = \frac{p(X | \vecT) \pi(\vecT)}{p(X)}.
\]
Помимо апостериорной плотности распределения нас 
часто интересует апостериорное прогнозное распределение для новых данных $X_{\mathrm{new}}$:
\[
p(X_{\mathrm{new}} | X) = \int p(X_{\mathrm{new}} | \vecT) p(\vecT | X) d\vecT.
\]

Часто нет необходимости рассматривать маргинальное распределение, так как оно не зависит от $\vecT$,
и с точностью до нормировочного коэффициента 
\[
p(\vecT | X) \propto p(X | \vecT) \pi(\vecT).
\]

Чтобы использовать введенные выше понятия
для решения реальных задач нужно ответить на два вопроса:
как выбрать априорное распределение и как вычислить все эти 
вероятности для конкретной модели.

Мы сосредоточимся на первом вопросе --- как выбрать априорное распределение?
По мере возможности постараемся осветить и второй.

\subsection{Классические критерии статистического оценивания}

Приведем классические критерии качества статистического оценивания из математической статистики.

\emph{Состоятельность.} Пусть $\hat{\theta}_\sS = \hat{\theta}(x_1, \dots, x_\sS)$.
Тогда оценка $\hat{\theta}_\sS$ называется состоятельной, если 
\[
\forall \theta \in \Theta \hat{\theta}_\sS \rightarrow^{p} \theta \text{ при } \sS \rightarrow \infty.
\]
То есть, оценка сходится к истинному значению параметра по вероятности, если размер выборки стремится к бесконечности.
\index{состоятельная оценка}

\emph{Скорость сходимости.} Нам часто важен не только сам факт сходимости, но и скорость сходимости.
То есть, нас интересует типичное значение величины $\|\hat{\theta}_\sS - \theta\|$ в зависимости от $\sS$. 
Иногда сходимость порядка $\frac{1}{\sqrt{\sS}}$ оказывается слишком медленной.

\emph{Несмещенность.} Оценку $\hat{\theta}$ назовем несмещенной, если $\bbE \hat{\theta} = \theta$.
\index{несмещенная оценка}

\emph{Эффективность.} Оценка будет эффективной, если она обеспечивает наилучшую возможную скорость сходимости.
В достаточно широком наборе моделей удается получить такие оценки.
\index{эффективная оценка}

Поясним введенные выше понятия с помощью примера.

\subsection{Пример использования Байесовского подхода}
\begin{example}
Пусть $x_i \sim \mathcal{N}(\theta, \sigma^2)$.
Есть выборка данных $\Sample = \{x_1, \ldots, x_{\sS}\}$, причем $x_i$ получены независимо, и они из одного и того же распределения.
Задача заключается в оценке выборке значения параметра $\theta$.

Будем использовать метод максимума правдоподобия:
\[
p(D| \theta) \rightarrow \max_{\theta}.
\]
Правдоподобие для такой модели имеет вид:
\[
p(D| \theta) = \prod_{i = 1}^{\sS} \frac{1}{\sqrt{2 \pi \sigma}} \exp \left(-\frac{1}{2 \sigma^2} (x_i - \theta)^2 \right).
\]

Максимизация логарифма правдоподобия --- что то же самое, что и максимизация правдоподобия --- эквивалентно задаче минимизации:
\[
\sum_{i = 1}^{\sS} (x_i - \theta)^2 \rightarrow \min_{\theta}.
\]
\index{метод максимума правдоподобия}
Дифференцируя и приравнивая к нуля производную, получаем оценку максимума правдоподобия (maximum likelihood estimate):
\[
\hat{\theta}_{MLE} = \frac{1}{\sS} \sum_{i = 1}^{\sS} x_i.
\]

Подсчитаем теперь математическое ожидание и дисперсию такой оценки:
\begin{align*}
\bbE \hat{\theta}_{MLE} &= \theta, \\
\bbV \hat{\theta}_{MLE} &= \frac{1}{n} \bbV x = \frac{\sigma^2}{n}. \\
\end{align*}
Таким образом, оценка максимума правдоподобия несмещенная и состоятельная.
Действительно, среднее значение совпадает с истинным значением параметра и выполнено условие состоятельности:
\[
\bbV \hat{\theta}_{MLE} \rightarrow 0, \sS \rightarrow \infty.
\]
Скорость сходимости $\bbV \|\hat{\theta}_{MLE} - \theta\|^2 \sim \frac{1}{\sS}$.

Более того, оказывается, такая оценка является эффективной.
Так как нормальное распределение принадлежит экспоненциальному семейству, то выполнено неравенство Рао-Крамера:
\[
\bbV \hat{\theta}_{MLE} \geq I_{\theta}^{-1},
\]
где $I_{\theta} = - \bbE  \left[\frac{\partial^2 p(D|\theta)}{\partial \theta^2} \right]$ --- информация Фишера\index{информация Фишера},
причем для модели из примера $I_{\theta} = \frac{\sS}{\sigma^2}$!

Таким образом, для оценки среднего значения нормального распределения выполнено, что
оценка максимума правдоподобия состоятельная, несмещенная и эффективная, причем 
дисперсия оценки убывает как $\frac{1}{\sS}$.

Посмотрим теперь на Байесовскую оценку.
Пусть задано априорное распределение $\pi(\theta) = \mathcal{N}(\mu, \sigma_{\theta}^2)$.
Тогда для апостериорного распределения выполнено, что
\[
p(\theta | D) \propto p(D| \theta) \pi(\theta).
\] 
Сделав несложные преобразования получаем, что
апостериорное распределение тоже будет нормальным, а именно:
\[
p(\theta | D) = \mathcal{N} \left(\frac{\frac{1}{\sigma^2} \sum_{i = 1}^{\sS} x_i + \frac{\mu}{\sigma_{\theta}^2}}{\frac{\sS}{\sigma^2} + \frac{1}{\sigma_{\theta}^2}}, \frac{1}{\frac{\sS}{\sigma^2} + \frac{1}{\sigma_{\theta}^2}} \right).
\] 
Легко видеть, что такая оценка состоятельна, но не является несмещенной и эффективной.
С другой стороны оказывается, что такая оценка \emph{асимптотически} несмещенная и эффективная.
\end{example}

 